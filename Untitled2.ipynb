{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '../data/generated/test_train_data'\n",
    "conll_path = '../data/basic_data/test_datasets'\n",
    "person_path = '../data/basic_data/p_e_m_data/persons.txt'\n",
    "voca_emb_dir = \"../data/generated/embeddings/word_ent_embs/\"\n",
    "ent_inlinks_path = \"../data/entityid_dictid_inlinks_uniq.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import json\n",
    "import dataset as D\n",
    "import argparse\n",
    "import utils as utils\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import pickle\n",
    "from ed_ranker import EDRanker\n",
    "import csv\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load csv\n",
      "370United News of India\n",
      "process coref\n",
      "load conll\n",
      "reorder mentions within the dataset\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(ctx_window=100, death_epoches=10, dev_f1_change_lr=0.928, dev_f1_start_order_learning=0.92, device=3, dropout_GCN=0.2, dropout_rate=0.2, eval_after_n_epochs=5, gamma=0.9, hid_dims=100, keep_ctx_ent=4, keep_p_e_m=4, learning_rate=0.0002, margin=0.01, method='SL', mode='train', model_path='Model/', n_cands_before_rank=50, n_epochs=500, n_not_inc=20, n_sample=10, one_entity_once=0, order='offset', output_path='Output1/', predict_epoches=100, prerank_ctx_window=50, search_entity_size=5, search_ment_size=5, seed=543, seq_len=0, tok_top_n=50, use_early_stop=False, edge_window=40, batch_maxsize=50)\n",
    "from imp import reload\n",
    "reload(D)\n",
    "import dataset as D\n",
    "conll = D.CoNLLDataset(datadir, conll_path, person_path, args.order, args.method, edge_window=args.edge_window, batch_maxsize=args.batch_maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.set_device(args.device)\n",
    "\n",
    "torch.manual_seed(args.seed)    # set random seed for cpu\n",
    "np.random.seed(args.seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(args.seed)   # set random seed for present GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "#F1_CSV_Path = args.output_path + args.method + \"_\" + args.order + \"_\" + \"f1.csv\"\n",
    "\n",
    "# F1_CSV_Path = args.output_path + args.method + \"_\" + args.order + \"_\" + str(args.tok_top_n) + \"-\" \\\n",
    "#               + str(args.tok_top_n4ent) + \"-\" + str(args.tok_top_n4word) + \"-\" + str(args.tok_top_n4inlink) + \"_\" \\\n",
    "#               + timestr + \"_\" + str(args.order_learning) + \"_\" + args.sort + \"_\" + str(args.seq_len) + str(args.isDynamic) + str(args.dca_method) + str(args.one_entity_once) + \"f1.csv\"\n",
    "\n",
    "F1_CSV_Path = args.output_path + args.method + \"_\" + timestr + \"_\" + \"f1.csv\"\n",
    "\n",
    "word_voca, word_embeddings = utils.load_voca_embs(voca_emb_dir + 'dict.word',\n",
    "                                                      voca_emb_dir + 'word_embeddings.npy')\n",
    "\n",
    "entity_voca, entity_embeddings = utils.load_voca_embs(voca_emb_dir + 'dict.entity',\n",
    "                                                        voca_emb_dir + 'entity_embeddings.npy')\n",
    "\n",
    "with open(ent_inlinks_path, 'rb') as f_pkl:\n",
    "    ent_inlinks_dict = pickle.load(f_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'hid_dims': args.hid_dims,\n",
    "          'emb_dims': entity_embeddings.shape[1],\n",
    "          'freeze_embs': True,\n",
    "          'tok_top_n': args.tok_top_n,\n",
    "          # 'tok_top_n4ment': args.tok_top_n4ment,\n",
    "          # 'tok_top_n4ent': args.tok_top_n4ent,\n",
    "          # 'tok_top_n4word': args.tok_top_n4word,\n",
    "          # 'tok_top_n4inlink': args.tok_top_n4inlink,\n",
    "          'margin': args.margin,\n",
    "          'word_voca': word_voca,\n",
    "          'entity_voca': entity_voca,\n",
    "          'word_embeddings': word_embeddings,\n",
    "          'entity_embeddings': entity_embeddings,\n",
    "          'entity_inlinks': ent_inlinks_dict,\n",
    "          'dr': args.dropout_rate,\n",
    "          'gdr': args.dropout_GCN,\n",
    "          'gamma': args.gamma,\n",
    "          # 'order_learning': args.order_learning,\n",
    "          # 'dca_method' : args.dca_method,\n",
    "          'f1_csv_path': F1_CSV_Path,\n",
    "          'seq_len': args.seq_len,\n",
    "          # 'isDynamic' : args.isDynamic,\n",
    "          'one_entity_once': args.one_entity_once,\n",
    "          'n_sample': args.n_sample,\n",
    "          'search_ment_size': args.search_ment_size,\n",
    "          'search_entity_size': args.search_entity_size,\n",
    "          'predict_epoches': args.predict_epoches,\n",
    "          'death_epoches': args.death_epoches,\n",
    "          'lr': args.learning_rate, \n",
    "          'n_epochs': args.n_epochs, \n",
    "          'use_early_stop' : args.use_early_stop,\n",
    "          'args': args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- create EDRanker model ---\n",
      "prerank model\n",
      "--- create NTEE model ---\n",
      "--- create AbstractWordEntity model ---\n",
      "main model\n",
      "create new model\n",
      "--- create MulRelRanker model ---\n",
      "--- create LocalCtxAttRanker model ---\n",
      "--- create AbstractWordEntity model ---\n"
     ]
    }
   ],
   "source": [
    "ranker = EDRanker(config=config)\n",
    "\n",
    "dev_datasets = [\n",
    "                # ('aida-train', conll.train),\n",
    "                ('aida-A', conll.testA, conll.testA_mlist, conll.testA_madj),\n",
    "                ('aida-B', conll.testB, conll.testB_mlist, conll.testB_madj),\n",
    "                ('msnbc', conll.msnbc, conll.msnbc_mlist, conll.msnbc_madj),\n",
    "                ('aquaint', conll.aquaint, conll.aquaint_mlist, conll.aquaint_madj),\n",
    "                ('ace2004', conll.ace2004, conll.ace2004_mlist, conll.ace2004_madj),\n",
    "                ('clueweb', conll.clueweb, conll.clueweb_mlist, conll.clueweb_madj),\n",
    "                ('wikipedia', conll.wikipedia, conll.wikipedia_mlist, conll.wikipedia_madj) \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(F1_CSV_Path, 'w') as f_csv_f1:\n",
    "    f1_csv_writer = csv.writer(f_csv_f1)\n",
    "    f1_csv_writer.writerow(['dataset', 'epoch', 'dynamic', 'F1 Score'])\n",
    "org_train_dataset = (conll.train, conll.train_mlist, conll.train_madj)\n",
    "org_dev_datasets = dev_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting training data\n",
      "recall 1.0\n",
      "#train docs 1014\n",
      "recall 0.9772490085577124\n",
      "aida-A #dev docs 236\n",
      "recall 0.9866220735785953\n",
      "aida-B #dev docs 247\n",
      "recall 0.9847560975609756\n",
      "msnbc #dev docs 24\n",
      "recall 0.9408528198074277\n",
      "aquaint #dev docs 50\n",
      "recall 0.914396887159533\n",
      "ace2004 #dev docs 35\n",
      "recall 0.9190424959655729\n",
      "clueweb #dev docs 320\n",
      "recall 0.93214074512123\n",
      "wikipedia #dev docs 321\n",
      "creating optimizer\n",
      "att_mat_diag\n",
      "tok_score_mat_diag\n",
      "type_emb\n",
      "gcned_mat_diag\n",
      "cnn.weight\n",
      "cnn.bias\n",
      "gcn.layer1.W\n",
      "gcn.layer2.W\n",
      "cnn_mgraph.weight\n",
      "cnn_mgraph.bias\n",
      "m_e_score.weight\n",
      "m_e_score.bias\n",
      "score_combine.0.weight\n",
      "score_combine.0.bias\n",
      "score_combine.3.weight\n",
      "score_combine.3.bias\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Local_ETHZ.vocabulary import Vocabulary\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import Local_ETHZ.dataset as D\n",
    "import Local_ETHZ.utils as utils\n",
    "import Local_ETHZ.ntee as ntee\n",
    "from Local_ETHZ.gcn.model import GCN\n",
    "from random import shuffle\n",
    "import torch.optim as optim\n",
    "from Local_ETHZ.abstract_word_entity import load as load_model\n",
    "from Local_ETHZ.mulrel_ranker import MulRelRanker\n",
    "from pprint import pprint\n",
    "from itertools import count\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "import ipdb\n",
    "\n",
    "ModelClass = MulRelRanker\n",
    "wiki_prefix = 'en.wikipedia.org/wiki/'\n",
    "debugging = True\n",
    "\n",
    "print('extracting training data')\n",
    "org_train_dataset, train_mlist, train_madj = org_train_dataset\n",
    "train_dataset = ranker.get_data_items(org_train_dataset, predict=False, isTrain=True)\n",
    "\n",
    "doc_names = list(train_mlist.keys())\n",
    "shuffle_list = list(zip(train_dataset, doc_names))\n",
    "shuffle(shuffle_list)\n",
    "train_dataset[:], doc_names[:] = zip(*shuffle_list)\n",
    "train_dataset_adj = utils.data_m_graph_build(train_dataset)\n",
    "\n",
    "gold_cand_to_idxs, gold_idx_to_cand, gold_e_adjs = ranker.gold_e_graph_build(train_dataset)\n",
    "print('#train docs', len(train_dataset))\n",
    "ranker.init_lr = config['lr']\n",
    "dev_datasets = []\n",
    "for dname, data, mlist, madj in org_dev_datasets:\n",
    "    dataitems = ranker.get_data_items(data, predict=True, isTrain=False)\n",
    "    dev_datasets.append((dname, dataitems, mlist, utils.data_m_graph_build(dataitems)))\n",
    "    print(dname, '#dev docs', len(dev_datasets[-1][1]))\n",
    "\n",
    "print('creating optimizer')\n",
    "optimizer = optim.Adam([p for p in ranker.model.parameters() if p.requires_grad], lr=config['lr'])\n",
    "\n",
    "for param_name, param in ranker.model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param_name)\n",
    "\n",
    "best_f1 = -1\n",
    "not_better_count = 0\n",
    "is_counting = False\n",
    "eval_after_n_epochs = ranker.args.eval_after_n_epochs\n",
    "\n",
    "order_learning = False\n",
    "# order_learning_count = 0\n",
    "\n",
    "rl_acc_threshold = 0.7\n",
    "\n",
    "# optimize the parameters within the disambiguation module first\n",
    "# self.model.switch_order_learning(0)\n",
    "best_aida_A_rlts = []\n",
    "best_aida_A_f1 = 0.\n",
    "best_aida_B_rlts = []\n",
    "best_aida_B_f1 = 0.\n",
    "best_ave_rlts = []\n",
    "best_ave_f1 = 0.\n",
    "\n",
    "ranker.run_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "cur_max_n_ment = 0\n",
    "\n",
    "# if order_learning:\n",
    "#     order_learning_count += 1\n",
    "#\n",
    "# if order_learning_count > 5:\n",
    "#     ranker.model.switch_order_learning(1)\n",
    "\n",
    "#for dc, batch in enumerate(train_dataset):  # each document is a minibatch\n",
    "dc = 0\n",
    "batch = train_dataset[dc]\n",
    "ranker.model.train()\n",
    "# print(\"dc:\",dc,\"start\")\n",
    "\n",
    "# convert data items to pytorch inputs\n",
    "token_ids = [m['context'][0] + m['context'][1]\n",
    "         if len(m['context'][0]) + len(m['context'][1]) > 0\n",
    "         else [ranker.model.word_voca.unk_id]\n",
    "         for m in batch]\n",
    "\n",
    "ment_ids = [m['ment_ids'] if len(m['ment_ids']) > 0\n",
    "        else [ranker.model.word_voca.unk_id]\n",
    "        for m in batch]\n",
    "\n",
    "entity_ids = Variable(torch.LongTensor([m['selected_cands']['cands'] for m in batch]).cuda())\n",
    "true_pos = Variable(torch.LongTensor([m['selected_cands']['true_pos'] for m in batch]).cuda())\n",
    "p_e_m = Variable(torch.FloatTensor([m['selected_cands']['p_e_m'] for m in batch]).cuda())\n",
    "entity_mask = Variable(torch.FloatTensor([m['selected_cands']['mask'] for m in batch]).cuda())\n",
    "\n",
    "# print('stage A')\n",
    "# entity_ids is n_ment * n_cand(k) indexes\n",
    "# for every entity candidate, we have a description\n",
    "desc_ids = torch.index_select(ranker.ent_desc, 0, entity_ids.view(-1)).view(entity_ids.size(0), entity_ids.size(1), -1)\n",
    "desc_mask = torch.index_select(ranker.desc_mask, 0, entity_ids.view(-1)).view(entity_ids.size(0), entity_ids.size(1), -1)\n",
    "\n",
    "mtype = Variable(torch.FloatTensor([m['mtype'] for m in batch]).cuda())\n",
    "etype = Variable(torch.FloatTensor([m['selected_cands']['etype'] for m in batch]).cuda())\n",
    "\n",
    "token_ids, token_mask = utils.make_equal_len(token_ids, ranker.model.word_voca.unk_id)\n",
    "token_ids = Variable(torch.LongTensor(token_ids).cuda())\n",
    "token_mask = Variable(torch.FloatTensor(token_mask).cuda())\n",
    "\n",
    "ment_ids, ment_mask = utils.make_equal_len(ment_ids, ranker.model.word_voca.unk_id)\n",
    "ment_ids = Variable(torch.LongTensor(ment_ids).cuda())\n",
    "ment_mask = Variable(torch.FloatTensor(ment_mask).cuda())\n",
    "# print('stage B')\n",
    "if ranker.args.method == \"SL\":\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# scores, _ = self.model.forward(token_ids, token_mask, entity_ids, entity_mask, p_e_m, mtype, etype,\n",
    "#                                ment_ids, ment_mask, desc_ids, desc_mask, gold=true_pos.view(-1, 1),\n",
    "#                                method=self.args.method,\n",
    "#                                isTrain=True, isDynamic=config['isDynamic'], isOrderLearning=order_learning,\n",
    "#                                isOrderFixed=True, isSort=self.args.sort)\n",
    "\n",
    "# scores, _ = self.model.forward(token_ids, token_mask, entity_ids, entity_mask, p_e_m, mtype, etype, ment_ids, ment_mask, desc_ids, desc_mask, gold=true_pos.view(-1, 1), method=self.args.method, isTrain=True)\n",
    "\n",
    "# sample_idx: n_ment * n_sample\n",
    "# copy_entity_mask: n_ment * n_cand\n",
    "sample_idx, copy_entity_mask = ranker.ment_neg_sample(ranker.n_sample, entity_ids, true_pos, entity_mask)\n",
    "\n",
    "n_ment, n_cand = entity_ids.size()\n",
    "cur_max_n_ment = max(cur_max_n_ment, n_ment)\n",
    "# print(\"cur_max_n_ment:\", cur_max_n_ment)\n",
    "e_cand_to_idxs = [[] for _ in range(n_ment) ]\n",
    "e_idx_to_cands = [[] for _ in range(n_ment) ]\n",
    "e_adjs = [[] for _ in range(n_ment) ]\n",
    "sele_cand = [m['selected_cands']['cands'] for m in batch]\n",
    "# print('stage C')\n",
    "for i in range(n_ment):\n",
    "    for j in range(ranker.n_sample):\n",
    "#                             if i in self.negsam_graph_cache and sample_idx[i][j] in self.negsam_graph_cache[i]:\n",
    "#                                 cand_to_idx, idx_to_cand, e_adj = negsam_graph_cache[i][sample_idx[i][j]]\n",
    "#                                 e_cand_to_idxs[i].append(cand_to_idx)\n",
    "#                                 e_idx_to_cands[i].append(idx_to_cand)\n",
    "#                                 e_adjs[i].append(e_adj)\n",
    "#                             else:\n",
    "        true_cands = copy.deepcopy(ranker.true_cands[dc])\n",
    "        true_cands[i] = sele_cand[i][sample_idx[i][j]]\n",
    "        cand_to_idx, idx_to_cand, e_adj = ranker.e_graph_build(true_cands)\n",
    "#                             if i not in self.negsam_graph_cache:\n",
    "#                                 self.negsam_graph_cache[i] = {}\n",
    "#                             self.negsam_graph_cache[i][sample_idx[i][j]] = cand_to_idx, idx_to_cand, e_adj\n",
    "        e_cand_to_idxs[i].append(cand_to_idx)\n",
    "        e_idx_to_cands[i].append(idx_to_cand)\n",
    "        e_adjs[i].append(e_adj)\n",
    "    e_cand_to_idxs[i].append(gold_cand_to_idxs[dc])\n",
    "    e_idx_to_cands[i].append(gold_idx_to_cand[dc])\n",
    "    e_adjs[i].append(gold_e_adjs[dc])\n",
    "\n",
    "# e_adjs = torch.LongTensor(e_adjs)\n",
    "# e_adjs: n_ment * n_sample * n_entity * n_entity\n",
    "# gold_e = (gold_cand_to_idxs[dc], gold_idx_to_cand[dc], gold_e_adjs[dc])\n",
    "#                     e_cand_to_idxs.append(gold_cand_to_idxs[dc])\n",
    "#                     e_idx_to_cands.append(gold_idx_to_cand[dc])\n",
    "#                     e_adjs.append(gold_e_adjs[dc])\n",
    "# print('stage D')\n",
    "new_adjs, new_node_cands, new_node_mask = utils.e_graph_batch_padding(e_cand_to_idxs, e_idx_to_cands, e_adjs, n_ment, ranker.n_sample)\n",
    "# new_adjs: n_ment * (n_sample+1) * n_node * n_node\n",
    "# new_node_cands: n_ment * (n_sample+1) * n_node\n",
    "# new_node_mask: n_ment * (n_sample+1) * n_node\n",
    "nega_e = (new_adjs, new_node_cands, new_node_mask)\n",
    "# print('stage E')\n",
    "cur_doc_name = doc_names[dc]\n",
    "scores, _ = ranker.model.forward(token_ids, token_mask, entity_ids, entity_mask, p_e_m, mtype, etype, ment_ids, ment_mask, desc_ids, desc_mask, train_mlist[cur_doc_name], train_dataset_adj[dc], nega_e, sample_idx, gold=true_pos.view(-1, 1), method=ranker.args.method, isTrain=True)\n",
    "# print('stage F')\n",
    "# if order_learning:\n",
    "#     _, targets = self.model.get_order_truth()\n",
    "#     targets = Variable(torch.LongTensor(targets).cuda())\n",
    "\n",
    "#     if scores.size(0) != targets.size(0):\n",
    "#         print(\"Size mismatch!\")\n",
    "#         break\n",
    "#     # why can model compute loss without aware of 'order_learing'\n",
    "#     loss = self.model.loss(scores, targets, method=self.args.method)\n",
    "# else:\n",
    "loss = ranker.model.loss(scores, torch.LongTensor([ranker.n_sample]*n_ment).cuda(), method=ranker.args.method)\n",
    "\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_([p for p in ranker.model.parameters() if p.requires_grad], max_norm=40, norm_type=2)\n",
    "for (name, p) in ranker.model.named_parameters():\n",
    "    if name == \"att_mat_diag\":\n",
    "        # print(name, p.grad.data)\n",
    "        if torch.isnan(p.grad.data).any():\n",
    "            print(\"att_mat_diag NAN detected\")\n",
    "            # ipdb.set_trace()\n",
    "            # p.grad.data.zero_()\n",
    "#                             elif torch.sum(p.grad.data)==0:\n",
    "#                                 print(\"sum 0 detected\")\n",
    "#                                 ipdb.set_trace()\n",
    "optimizer.step()\n",
    "ranker.model.regularize(max_norm=20)\n",
    "\n",
    "loss = loss.cpu().data.numpy()\n",
    "total_loss += loss\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_f1 = 0.\n",
    "test_f1 = 0.\n",
    "ave_f1 = 0.\n",
    "if rl_acc_threshold < 0.92:\n",
    "    rl_acc_threshold += 0.02\n",
    "temp_rlt = []\n",
    "#self.records[e] = dict()\n",
    "# for di, (dname, data, mlist, madj) in enumerate(dev_datasets):\n",
    "di = 0\n",
    "dname, data, mlist, madj = dev_datasets[di]\n",
    "if dname == 'aida-B':\n",
    "    ranker.rt_flag = True\n",
    "else:\n",
    "    ranker.rt_flag = False\n",
    "# predictions = self.predict(data, config['isDynamic'], order_learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulRelRanker(\n",
       "  (word_embeddings): Embedding(492408, 300)\n",
       "  (entity_embeddings): Embedding(274475, 300)\n",
       "  (local_ctx_dr): Dropout(p=0, inplace=False)\n",
       "  (cnn): Conv1d(300, 64, kernel_size=(3,), stride=(1,))\n",
       "  (gcn): GCN(\n",
       "    (layer1): singleGCNLayer()\n",
       "    (layer2): singleGCNLayer()\n",
       "  )\n",
       "  (cnn_mgraph): Conv1d(300, 300, kernel_size=(5,), stride=(1,))\n",
       "  (m_e_score): Linear(in_features=600, out_features=1, bias=True)\n",
       "  (score_combine): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions = ranker.predict(data, mlist, madj)\n",
    "# self.records[e][dname] = self.record\n",
    "\n",
    "search_ment_size = ranker.search_ment_size\n",
    "search_entity_size = ranker.search_entity_size\n",
    "predict_epoches = ranker.predict_epoches\n",
    "death_epoches = ranker.death_epoches\n",
    "predictions = {items[0]['doc_name']: [] for items in data}\n",
    "ranker.model.eval()\n",
    "#self.record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = 0\n",
    "batch = data[dc]\n",
    "# self.model.doc_predict_restore = False\n",
    "start_time = time.time()\n",
    "token_ids = [m['context'][0] + m['context'][1]\n",
    "             if len(m['context'][0]) + len(m['context'][1]) > 0\n",
    "             else [ranker.model.word_voca.unk_id]\n",
    "             for m in batch]\n",
    "\n",
    "ment_ids = [m['ment_ids'] if len(m['ment_ids']) > 0\n",
    "            else [ranker.model.word_voca.unk_id]\n",
    "            for m in batch]\n",
    "\n",
    "total_candidates = sum([len(m['selected_cands']['cands']) for m in batch])\n",
    "\n",
    "entity_ids = Variable(torch.LongTensor([m['selected_cands']['cands'] for m in batch]).cuda())\n",
    "p_e_m = Variable(torch.FloatTensor([m['selected_cands']['p_e_m'] for m in batch]).cuda())\n",
    "entity_mask = Variable(torch.FloatTensor([m['selected_cands']['mask'] for m in batch]).cuda())\n",
    "true_pos = Variable(torch.LongTensor([m['selected_cands']['true_pos'] for m in batch]).cuda())\n",
    "\n",
    "token_ids, token_mask = utils.make_equal_len(token_ids, ranker.model.word_voca.unk_id)\n",
    "\n",
    "token_ids = Variable(torch.LongTensor(token_ids).cuda())\n",
    "token_mask = Variable(torch.FloatTensor(token_mask).cuda())\n",
    "\n",
    "desc_ids = torch.index_select(ranker.ent_desc, 0, entity_ids.view(-1)).view(entity_ids.size(0),entity_ids.size(1), -1)\n",
    "desc_mask = torch.index_select(ranker.desc_mask, 0, entity_ids.view(-1)).view(entity_ids.size(0),entity_ids.size(1), -1)\n",
    "ment_ids, ment_mask = utils.make_equal_len(ment_ids, ranker.model.word_voca.unk_id)\n",
    "ment_ids = Variable(torch.LongTensor(ment_ids).cuda())\n",
    "ment_mask = Variable(torch.FloatTensor(ment_mask).cuda())\n",
    "\n",
    "mtype = Variable(torch.FloatTensor([m['mtype'] for m in batch]).cuda())\n",
    "etype = Variable(torch.FloatTensor([m['selected_cands']['etype'] for m in batch]).cuda())\n",
    "\n",
    "n_ments, n_cands = entity_ids.size()\n",
    "# the val in cur_cand_idxs should be in 0~n_cand\n",
    "cur_cand_idxs = torch.multinomial(entity_mask, 1, replacement=False).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_cnt = 0\n",
    "pde = 0\n",
    "cur_cands = torch.gather(entity_ids, 1, cur_cand_idxs.unsqueeze(1))\n",
    "list_cands = cur_cands.squeeze(1).cpu().numpy().tolist()\n",
    "# cur_cands: n_ment * 1\n",
    "cand_to_idx, idx_to_cand, e_adj = ranker.e_graph_build(list_cands)\n",
    "idx_to_cand = torch.LongTensor(idx_to_cand).cuda()\n",
    "the_mask = torch.ones(idx_to_cand.size(0)).cuda()\n",
    "nega_e = (torch.LongTensor(e_adj).cuda(), idx_to_cand, the_mask)\n",
    "cur_scores, _ = ranker.model.forward(token_ids, token_mask, entity_ids, entity_mask, p_e_m, mtype, etype, ment_ids, ment_mask, desc_ids, desc_mask, None, madj[dc], nega_e, cur_cand_idxs, gold=None, method=\"SL\", isTrain=False, chosen_ment=False)\n",
    "assert cur_scores.size(0) == n_ments\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cur_scores.squeeze(1))\n",
    "# print(entity_mask.sum(dim=1))\n",
    "# print(cur_scores.size(0))\n",
    "# print(entity_mask.size(0))\n",
    "# print(n_ments)\n",
    "# print(cur_cand_idxs.size())\n",
    "# print(min(n_cands, search_entity_size)-1)\n",
    "# ab = torch.LongTensor([0,1,2,3,4]).cuda()\n",
    "# print(entity_mask[ab])\n",
    "# print(batch[3]['selected_cands']['cands'])\n",
    "# print(torch.multinomial(entity_mask[ab], min(n_cands, search_entity_size)-1, replacement=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_scores, small_idxs = torch.topk(cur_scores.squeeze(1), min(search_ment_size, n_ments), largest=False, sorted=True)\n",
    "\n",
    "random_new_cand_idx = torch.cat([cur_cand_idxs[small_idxs].unsqueeze(1), torch.multinomial(entity_mask[small_idxs], min(n_cands, search_entity_size)-1, replacement=True)], dim=1)\n",
    "# small_scores: search_ment_size\n",
    "# random_new_cand_idx: search_ment_size * search_entity_size, value in 0~n_cands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12, 28, 19, 34, 10], device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "print(small_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_cand_to_idxs = [[] for _ in range(min(search_ment_size, n_ments)) ]\n",
    "e_idx_to_cands = [[] for _ in range(min(search_ment_size, n_ments)) ]\n",
    "e_adjs = [[] for _ in range(min(search_ment_size, n_ments)) ]\n",
    "    \n",
    "for i in range(min(search_ment_size, n_ments)):\n",
    "    for j in range(min(n_cands, search_entity_size)):\n",
    "        true_cands = copy.deepcopy(list_cands)\n",
    "        which_ment = small_idxs[i]\n",
    "        true_cands[which_ment] = entity_ids[i][random_new_cand_idx[i][j]]\n",
    "        cand_to_idx, idx_to_cand, e_adj = ranker.e_graph_build(true_cands)\n",
    "        e_cand_to_idxs[i].append(cand_to_idx)\n",
    "        e_idx_to_cands[i].append(idx_to_cand)\n",
    "        e_adjs[i].append(e_adj)\n",
    "new_adjs, new_node_cands, new_node_mask = utils.e_graph_batch_padding(e_cand_to_idxs, e_idx_to_cands, e_adjs, min(search_ment_size, n_ments), min(n_cands, search_entity_size)-1)\n",
    "nega_e = (new_adjs, new_node_cands, new_node_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scores, _ = ranker.model.forward(token_ids, token_mask, entity_ids, entity_mask, p_e_m, mtype, etype, ment_ids, ment_mask, desc_ids, desc_mask, None, madj[dc], nega_e, random_new_cand_idx, gold=None, method=\"SL\", isTrain=False, chosen_ment=small_idxs)\n",
    "# new_scores: search_ment_size * search_entity_size\n",
    "\n",
    "big_idxs_in_new = torch.argmax(new_scores, dim=1)\n",
    "# big_idxs = random_new_cand_idx[big_idxs_in_new]\n",
    "big_idxs = torch.gather(random_new_cand_idx, 1, big_idxs_in_new.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "if big_idxs_in_new.sum() == 0:\n",
    "    death_cnt = death_cnt + 1\n",
    "#     if death_cnt > death_epoches:\n",
    "#         break\n",
    "else:\n",
    "    cur_cand_idxs[small_idxs] = big_idxs\n",
    "    death_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list_cands)\n",
    "# print(small_idxs)\n",
    "# print(random_new_cand_idx)\n",
    "# print(new_node_cands.size())\n",
    "# print(new_adjs.size())\n",
    "# print(new_node_mask.size())\n",
    "# adc = torch.randn(5,5)\n",
    "# dd = torch.LongTensor([4,2,0,0,1])\n",
    "# print(adc)\n",
    "# print(torch.gather(adc, 1, dd.unsqueeze(1)).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ids = cur_cand_idxs\n",
    "end_time = time.time()\n",
    "if ranker.rt_flag:\n",
    "    ranker.run_time.append([total_candidates, end_time-start_time])\n",
    "# if order_learning:\n",
    "#     pred_entities = list()\n",
    "\n",
    "#     decision_order, _ = self.model.get_order_truth()\n",
    "\n",
    "#     for mi, m in enumerate(batch):\n",
    "#         pi = pred_ids[decision_order.index(mi)]\n",
    "#         if m['selected_cands']['mask'][pi] == 1:\n",
    "#             pred_entities.append(m['selected_cands']['named_cands'][pi])\n",
    "#         else:\n",
    "#             if m['selected_cands']['mask'][0] == 1:\n",
    "#                 pred_entities.append(m['selected_cands']['named_cands'][0])\n",
    "#             else:\n",
    "#                 pred_entities.append('NIL')\n",
    "# else:\n",
    "pred_entities = [m['selected_cands']['named_cands'][i] if m['selected_cands']['mask'][i] == 1\n",
    "                 else (m['selected_cands']['named_cands'][0] if m['selected_cands']['mask'][0] == 1 else 'NIL')\n",
    "                 for (i, m) in zip(pred_ids, batch)]\n",
    "\n",
    "doc_names = [m['doc_name'] for m in batch]\n",
    "ranker.added_words = []\n",
    "ranker.added_ents = []\n",
    "if ranker.seq_len>0 and self.one_entity_once:\n",
    "    #self.added_words.append([self.word_vocab.id2word[idx] for idx in self.model.added_words[-1]])\n",
    "    #self.added_ents.append([self.ent_vocab.id2word[idx] for idx in self.model.added_ents[-1]])\n",
    "    predictions[doc_names[-1]].append({'pred': (pred_entities[-1], 0.)})\n",
    "else:\n",
    "    # for ids in self.model.added_words:\n",
    "    #     self.added_words.append([self.word_vocab.id2word[idx] for idx in ids])\n",
    "    # for ids in self.model.added_ents:\n",
    "    #     self.added_ents.append([self.ent_vocab.id2word[idx] for idx in ids])\n",
    "    for dname, entity in zip(doc_names, pred_entities):\n",
    "        predictions[dname].append({'pred': (entity, 0.)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947testa 947testa micro F1: 0.00496072757337743\n"
     ]
    }
   ],
   "source": [
    "f1 = D.eval(org_dev_datasets[di][1], predictions)\n",
    "\n",
    "print(dname, 'micro F1: ' + str(f1))\n",
    "\n",
    "# with open(ranker.output_path, 'a') as eval_csv_f1:\n",
    "#     eval_f1_csv_writer = csv.writer(eval_csv_f1)\n",
    "#     eval_f1_csv_writer.writerow([dname, e, 0, f1])\n",
    "temp_rlt.append([dname, f1])\n",
    "if dname == 'aida-A':\n",
    "    dev_f1 = f1\n",
    "if dname == 'aida-B':\n",
    "    test_f1 = f1\n",
    "ave_f1 += f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
